

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>btgym.algorithms package &mdash; BTGym 0.0.7 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="BTGym 0.0.7 documentation" href="index.html"/>
        <link rel="next" title="btgym.algorithms.nn.losses module" href="btgym.algorithms.nn.html"/>
        <link rel="prev" title="btgym.datafeed package" href="btgym.datafeed.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> BTGym
          

          
          </a>

          
            
            
              <div class="version">
                0.0.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Package Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#quickstart">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#problem-definition">Problem definition</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#environment-engine-description">Environment engine description</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#data-flow-structure">Data flow structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#a3c-framework-description">A3C framework description</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.envs.html">btgym.envs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html">btgym.dataserver module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html#module-btgym.server">btgym.server module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html#module-btgym.spaces">btgym.spaces module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.strategy.html">btgym.strategy package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.monitor.html">btgym.monitor package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.rendering.html">btgym.rendering package</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.datafeed.html">btgym.datafeed package</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">btgym.algorithms package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#btgym-algorithms-nn-subpackage">btgym.algorithms.nn subpackage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.nn.html">btgym.algorithms.nn.losses module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.nn.html#module-btgym.algorithms.nn.networks">btgym.algorithms.nn.networks module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.nn.html#module-btgym.algorithms.nn.layers">btgym.algorithms.nn.layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.nn.html#module-btgym.algorithms.nn.ae">btgym.algorithms.nn.ae module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#btgym-algorithms-policy-subpackage">btgym.algorithms.policy subpackage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.policy.html">btgym.algorithms.policy.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.policy.html#module-btgym.algorithms.policy.stacked_lstm">btgym.algorithms.policy.stacked_lstm module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#btgym-algorithms-runner-subpackage">btgym.algorithms.runner subpackage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.runner.html">btgym.algorithms.runner.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.runner.html#module-btgym.algorithms.runner.threadrunner">btgym.algorithms.runner.threadrunner module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btgym.algorithms.runner.html#module-btgym.algorithms.runner.synchro">btgym.algorithms.runner.synchro module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.launcher">btgym.algorithms.launcher module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.worker">btgym.algorithms.worker module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.aac">btgym.algorithms.aac module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.rollout">btgym.algorithms.rollout module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.memory">btgym.algorithms.memory module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.envs">btgym.algorithms.envs module</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.research.html">btgym.research package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BTGym</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>btgym.algorithms package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/btgym.algorithms.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="btgym-algorithms-package">
<h1>btgym.algorithms package<a class="headerlink" href="#btgym-algorithms-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="btgym-algorithms-nn-subpackage">
<h2>btgym.algorithms.nn subpackage<a class="headerlink" href="#btgym-algorithms-nn-subpackage" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.nn.html">btgym.algorithms.nn.losses module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.nn.html#module-btgym.algorithms.nn.networks">btgym.algorithms.nn.networks module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.nn.html#module-btgym.algorithms.nn.layers">btgym.algorithms.nn.layers module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.nn.html#module-btgym.algorithms.nn.ae">btgym.algorithms.nn.ae module</a></li>
</ul>
</div>
</div>
<div class="section" id="btgym-algorithms-policy-subpackage">
<h2>btgym.algorithms.policy subpackage<a class="headerlink" href="#btgym-algorithms-policy-subpackage" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.policy.html">btgym.algorithms.policy.base module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.policy.html#module-btgym.algorithms.policy.stacked_lstm">btgym.algorithms.policy.stacked_lstm module</a></li>
</ul>
</div>
</div>
<div class="section" id="btgym-algorithms-runner-subpackage">
<h2>btgym.algorithms.runner subpackage<a class="headerlink" href="#btgym-algorithms-runner-subpackage" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.runner.html">btgym.algorithms.runner.base module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.runner.html#module-btgym.algorithms.runner.threadrunner">btgym.algorithms.runner.threadrunner module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.algorithms.runner.html#module-btgym.algorithms.runner.synchro">btgym.algorithms.runner.synchro module</a></li>
</ul>
</div>
</div>
<div class="section" id="module-btgym.algorithms.launcher">
<span id="btgym-algorithms-launcher-module"></span><h2>btgym.algorithms.launcher module<a class="headerlink" href="#module-btgym.algorithms.launcher" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-btgym.algorithms.worker">
<span id="btgym-algorithms-worker-module"></span><h2>btgym.algorithms.worker module<a class="headerlink" href="#module-btgym.algorithms.worker" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.worker.FastSaver">
<em class="property">class </em><code class="descclassname">btgym.algorithms.worker.</code><code class="descname">FastSaver</code><span class="sig-paren">(</span><em>var_list=None</em>, <em>reshape=False</em>, <em>sharded=False</em>, <em>max_to_keep=5</em>, <em>keep_checkpoint_every_n_hours=10000.0</em>, <em>name=None</em>, <em>restore_sequentially=False</em>, <em>saver_def=None</em>, <em>builder=None</em>, <em>defer_build=False</em>, <em>allow_empty=False</em>, <em>write_version=2</em>, <em>pad_step_number=False</em>, <em>save_relative_paths=False</em>, <em>filename=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#FastSaver"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.FastSaver" title="Permalink to this definition">¶</a></dt>
<dd><p>Disables write_meta_graph argument,
which freezes entire process and is mostly useless.</p>
<p>Creates a <cite>Saver</cite>.</p>
<p>The constructor adds ops to save and restore variables.</p>
<p><cite>var_list</cite> specifies the variables that will be saved and restored. It can
be passed as a <cite>dict</cite> or a list:</p>
<ul class="simple">
<li>A <cite>dict</cite> of names to variables: The keys are the names that will be
used to save or restore the variables in the checkpoint files.</li>
<li>A list of variables: The variables will be keyed with their op name in
the checkpoint files.</li>
</ul>
<p>For example:</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
v1 = tf.Variable(…, name=’v1’)
v2 = tf.Variable(…, name=’v2’)</p>
<p># Pass the variables as a dict:
saver = tf.train.Saver({‘v1’: v1, ‘v2’: v2})</p>
<p># Or pass them as a list.
saver = tf.train.Saver([v1, v2])
# Passing a list is equivalent to passing a dict with the variable op names
# as keys:
saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>The optional <cite>reshape</cite> argument, if <cite>True</cite>, allows restoring a variable from
a save file where the variable had a different shape, but the same number
of elements and type.  This is useful if you have reshaped a variable and
want to reload it from an older checkpoint.</p>
<p>The optional <cite>sharded</cite> argument, if <cite>True</cite>, instructs the saver to shard
checkpoints per device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>var_list</strong> – A list of <cite>Variable</cite>/<cite>SaveableObject</cite>, or a dictionary mapping
names to <cite>SaveableObject`s. If `None</cite>, defaults to the list of all
saveable objects.</li>
<li><strong>reshape</strong> – If <cite>True</cite>, allows restoring parameters from a checkpoint
where the variables have a different shape.</li>
<li><strong>sharded</strong> – If <cite>True</cite>, shard the checkpoints, one per device.</li>
<li><strong>max_to_keep</strong> – Maximum number of recent checkpoints to keep.
Defaults to 5.</li>
<li><strong>keep_checkpoint_every_n_hours</strong> – How often to keep checkpoints.
Defaults to 10,000 hours.</li>
<li><strong>name</strong> – String.  Optional name to use as a prefix when adding operations.</li>
<li><strong>restore_sequentially</strong> – A <cite>Bool</cite>, which if true, causes restore of different
variables to happen sequentially within each device.  This can lower
memory usage when restoring very large models.</li>
<li><strong>saver_def</strong> – Optional <cite>SaverDef</cite> proto to use instead of running the
builder. This is only useful for specialty code that wants to recreate
a <cite>Saver</cite> object for a previously built <cite>Graph</cite> that had a <cite>Saver</cite>.
The <cite>saver_def</cite> proto should be the one returned by the
<cite>as_saver_def()</cite> call of the <cite>Saver</cite> that was created for that <cite>Graph</cite>.</li>
<li><strong>builder</strong> – Optional <cite>SaverBuilder</cite> to use if a <cite>saver_def</cite> was not provided.
Defaults to <cite>BulkSaverBuilder()</cite>.</li>
<li><strong>defer_build</strong> – If <cite>True</cite>, defer adding the save and restore ops to the
<cite>build()</cite> call. In that case <cite>build()</cite> should be called before
finalizing the graph or using the saver.</li>
<li><strong>allow_empty</strong> – If <cite>False</cite> (default) raise an error if there are no
variables in the graph. Otherwise, construct the saver anyway and make
it a no-op.</li>
<li><strong>write_version</strong> – controls what format to use when saving checkpoints.  It
also affects certain filepath matching logic.  The V2 format is the
recommended choice: it is much more optimized than V1 in terms of
memory required and latency incurred during restore.  Regardless of
this flag, the Saver is able to restore from both V2 and V1 checkpoints.</li>
<li><strong>pad_step_number</strong> – if True, pads the global step number in the checkpoint
filepaths to some fixed width (8 by default).  This is turned off by
default.</li>
<li><strong>save_relative_paths</strong> – If <cite>True</cite>, will write relative paths to the
checkpoint state file. This is needed if the user wants to copy the
checkpoint directory and reload from the copied directory.</li>
<li><strong>filename</strong> – If known at graph construction time, filename used for variable
loading/saving.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.7)"><code class="xref py py-exc docutils literal"><span class="pre">TypeError</span></code></a> – If <cite>var_list</cite> is invalid.</li>
<li><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.7)"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code></a> – If any of the keys or values in <cite>var_list</cite> are not unique.</li>
<li><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.7)"><code class="xref py py-exc docutils literal"><span class="pre">RuntimeError</span></code></a> – If eager execution is enabled and`var_list` does not specify
a list of varialbes to save.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>&#64;compatibility(eager)
When eager execution is enabled, <cite>var_list</cite> must specify a <cite>list</cite> or <cite>dict</cite>
of variables to save. Otherwise, a <cite>RuntimeError</cite> will be raised.
&#64;end_compatibility</p>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.worker.Worker">
<em class="property">class </em><code class="descclassname">btgym.algorithms.worker.</code><code class="descname">Worker</code><span class="sig-paren">(</span><em>env_config</em>, <em>policy_config</em>, <em>trainer_config</em>, <em>cluster_spec</em>, <em>job_name</em>, <em>task</em>, <em>log_dir</em>, <em>log_ckpt_subdir</em>, <em>initial_ckpt_dir</em>, <em>save_secs</em>, <em>log_level</em>, <em>max_env_steps</em>, <em>random_seed=None</em>, <em>render_last_env=False</em>, <em>test_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#Worker"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.Worker" title="Permalink to this definition">¶</a></dt>
<dd><p>Distributed tf worker class.</p>
<p>Sets up environment, trainer and starts training process in supervised session.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_config</strong> – environment class_config_dict.</li>
<li><strong>policy_config</strong> – model policy estimator class_config_dict.</li>
<li><strong>trainer_config</strong> – algorithm class_config_dict.</li>
<li><strong>cluster_spec</strong> – tf.cluster specification.</li>
<li><strong>job_name</strong> – worker or parameter server.</li>
<li><strong>task</strong> – integer number, 0 is chief worker.</li>
<li><strong>log_dir</strong> – path for tb summaries and current checkpoints.</li>
<li><strong>log_ckpt_subdir</strong> – log_dir subdirectory to store current checkpoints</li>
<li><strong>initial_ckpt_dir</strong> – path for checkpoint to load as pre-trained model.</li>
<li><strong>save_secs</strong> – int, save model checkpoint every N secs.</li>
<li><strong>log_level</strong> – int, logbook.level</li>
<li><strong>max_env_steps</strong> – number of environment steps to run training on</li>
<li><strong>random_seed</strong> – int or None</li>
<li><strong>render_last_env</strong> – bool, if True - render enabled for last environment in a list; first otherwise</li>
<li><strong>test_mode</strong> – if True - use Atari mode, BTGym otherwise.</li>
<li><strong>Note</strong> – <ul>
<li><dl class="first docutils">
<dt>Conventional <cite>self.global_step</cite> refers to number of environment steps,</dt>
<dd>summarized over all environment instances, not to number of policy optimizer train steps.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Every worker can run several environments in parralell, as specified by <a href="#id9"><span class="problematic" id="id10">`</span></a>cluster_config’[‘num_envs’].</dt>
<dd>If use 4 forkers and num_envs=4 =&gt; total number of environments is 16. Every env instance has
it’s own ThreadRunner process.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>When using replay memory, keep in mind that every ThreadRunner is keeping it’s own replay memory,</dt>
<dd>If memory_size = 2000, num_workers=4, num_envs=4 =&gt; total replay memory size equals 32 000 frames.</dd>
</dl>
</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.worker.Worker.run">
<code class="descname">run</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#Worker.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.Worker.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Worker runtime body.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.aac">
<span id="btgym-algorithms-aac-module"></span><h2>btgym.algorithms.aac module<a class="headerlink" href="#module-btgym.algorithms.aac" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.aac.BaseAAC">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">BaseAAC</code><span class="sig-paren">(</span><em>env</em>, <em>task</em>, <em>policy_config</em>, <em>log_level</em>, <em>name='AAC'</em>, <em>on_policy_loss=&lt;function aac_loss_def&gt;</em>, <em>off_policy_loss=&lt;function aac_loss_def&gt;</em>, <em>vr_loss=&lt;function value_fn_loss_def&gt;</em>, <em>rp_loss=&lt;function rp_loss_def&gt;</em>, <em>pc_loss=&lt;function pc_loss_def&gt;</em>, <em>runner_config=None</em>, <em>runner_fn_ref=&lt;function BaseEnvRunnerFn&gt;</em>, <em>cluster_spec=None</em>, <em>random_seed=None</em>, <em>model_gamma=0.99</em>, <em>model_gae_lambda=1.0</em>, <em>model_beta=0.01</em>, <em>opt_max_env_steps=10000000</em>, <em>opt_decay_steps=None</em>, <em>opt_end_learn_rate=None</em>, <em>opt_learn_rate=0.0001</em>, <em>opt_decay=0.99</em>, <em>opt_momentum=0.0</em>, <em>opt_epsilon=1e-08</em>, <em>rollout_length=20</em>, <em>time_flat=False</em>, <em>episode_train_test_cycle=(1</em>, <em>0)</em>, <em>episode_summary_freq=2</em>, <em>env_render_freq=10</em>, <em>model_summary_freq=100</em>, <em>test_mode=False</em>, <em>replay_memory_size=2000</em>, <em>replay_batch_size=None</em>, <em>replay_rollout_length=None</em>, <em>use_off_policy_aac=False</em>, <em>use_reward_prediction=False</em>, <em>use_pixel_control=False</em>, <em>use_value_replay=False</em>, <em>rp_lambda=1.0</em>, <em>pc_lambda=1.0</em>, <em>vr_lambda=1.0</em>, <em>off_aac_lambda=1</em>, <em>gamma_pc=0.9</em>, <em>rp_reward_threshold=0.1</em>, <em>rp_sequence_size=3</em>, <em>clip_epsilon=0.1</em>, <em>num_epochs=1</em>, <em>pi_prime_update_period=1</em>, <em>global_step_op=None</em>, <em>global_episode_op=None</em>, <em>inc_episode_op=None</em>, <em>_use_global_network=True</em>, <em>_use_target_policy=False</em>, <em>_use_local_memory=False</em>, <em>aux_render_modes=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC" title="Permalink to this definition">¶</a></dt>
<dd><p>Base Asynchronous Advantage Actor Critic algorithm framework class with auxiliary control tasks and
option to run several instances of environment for every worker in vectorized fashion, PAAC-like.
Can be configured to run with different losses and policies.</p>
<p>Auxiliary tasks implementation borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:
<a class="reference external" href="https://miyosuda.github.io/">https://miyosuda.github.io/</a>
<a class="reference external" href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></p>
<p>Original A3C code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Papers:
<a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a>
<a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance or list of instances</li>
<li><strong>task</strong> – int, parent worker id</li>
<li><strong>policy_config</strong> – policy estimator class and configuration dictionary</li>
<li><strong>log_level</strong> – int, logbook.level</li>
<li><strong>name</strong> – str, class-wide name-scope</li>
<li><strong>on_policy_loss</strong> – callable returning tensor holding on_policy training loss graph and summaries</li>
<li><strong>off_policy_loss</strong> – callable returning tensor holding off_policy training loss graph and summaries</li>
<li><strong>vr_loss</strong> – callable returning tensor holding value replay loss graph and summaries</li>
<li><strong>rp_loss</strong> – callable returning tensor holding reward prediction loss graph and summaries</li>
<li><strong>pc_loss</strong> – callable returning tensor holding pixel_control loss graph and summaries</li>
<li><strong>runner_config</strong> – runner class and configuration dictionary,</li>
<li><strong>runner_fn_ref</strong> – callable defining environment runner execution logic,
valid only if no ‘runner_config’ arg is provided</li>
<li><strong>cluster_spec</strong> – dict, full training cluster spec (may be used by meta-trainer)</li>
<li><strong>random_seed</strong> – int or None</li>
<li><strong>model_gamma</strong> – scalar, gamma discount factor</li>
<li><strong>model_gae_lambda</strong> – scalar, GAE lambda</li>
<li><strong>model_beta</strong> – entropy regularization beta, scalar or [high_bound, low_bound] for log_uniform.</li>
<li><strong>opt_max_env_steps</strong> – int, total number of environment steps to run training on.</li>
<li><strong>opt_decay_steps</strong> – int, learn ratio decay steps, in number of environment steps.</li>
<li><strong>opt_end_learn_rate</strong> – scalar, final learn rate</li>
<li><strong>opt_learn_rate</strong> – start learn rate, scalar or [high_bound, low_bound] for log_uniform distr.</li>
<li><strong>opt_decay</strong> – scalar, optimizer decay, if apll.</li>
<li><strong>opt_momentum</strong> – scalar, optimizer momentum, if apll.</li>
<li><strong>opt_epsilon</strong> – scalar, optimizer epsilon</li>
<li><strong>rollout_length</strong> – int, on-policy rollout length</li>
<li><strong>time_flat</strong> – bool, flatten rnn time-steps in rollouts while training - see <cite>Notes</cite> below</li>
<li><strong>episode_train_test_cycle</strong> – tuple or list as (train_number, test_number), def=(1,0): enables infinite
loop such as: run <cite>train_number</cite> of train data episodes,
than <cite>test_number</cite> of test data episodes, repeat. Should be consistent
with provided dataset parameters (test data should exist if <cite>test_number &gt; 0</cite>)</li>
<li><strong>episode_summary_freq</strong> – int, write episode summary for every i’th episode</li>
<li><strong>env_render_freq</strong> – int, write environment rendering summary for every i’th train step</li>
<li><strong>model_summary_freq</strong> – int, write model summary for every i’th train step</li>
<li><strong>test_mode</strong> – bool, True: Atari, False: BTGym</li>
<li><strong>replay_memory_size</strong> – int, in number of experiences</li>
<li><strong>replay_batch_size</strong> – int, mini-batch size for off-policy training, def = 1</li>
<li><strong>replay_rollout_length</strong> – int off-policy rollout length by def. equals on_policy_rollout_length</li>
<li><strong>use_off_policy_aac</strong> – bool, use full AAC off-policy loss instead of Value-replay</li>
<li><strong>use_reward_prediction</strong> – bool, use aux. off-policy reward prediction task</li>
<li><strong>use_pixel_control</strong> – bool, use aux. off-policy pixel control task</li>
<li><strong>use_value_replay</strong> – bool, use aux. off-policy value replay task (not used if use_off_policy_aac=True)</li>
<li><strong>rp_lambda</strong> – reward prediction loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>pc_lambda</strong> – pixel control loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>vr_lambda</strong> – value replay loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>off_aac_lambda</strong> – off-policy AAC loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>gamma_pc</strong> – NOT USED</li>
<li><strong>rp_reward_threshold</strong> – scalar, reward prediction classification threshold, above which reward is ‘non-zero’</li>
<li><strong>rp_sequence_size</strong> – int, reward prediction sample size, in number of experiences</li>
<li><strong>clip_epsilon</strong> – scalar, PPO: surrogate L^clip epsilon</li>
<li><strong>num_epochs</strong> – int, num. of SGD runs for every train step, val. &gt; 1 should be used with caution.</li>
<li><strong>pi_prime_update_period</strong> – int, PPO: pi to pi_old update period in number of train steps, def: 1</li>
<li><strong>global_step_op</strong> – external tf.variable holding global step counter</li>
<li><strong>global_episode_op</strong> – external tf.variable holding global episode counter</li>
<li><strong>inc_episode_op</strong> – external tf.op incrementing global step counter</li>
<li><strong>_use_global_network</strong> – bool, either to use parameter server policy instance</li>
<li><strong>_use_target_policy</strong> – bool, PPO: use target policy (aka pi_old), delayed by <cite>pi_prime_update_period</cite> delay</li>
<li><strong>_use_local_memory</strong> – bool: use in-process replay memory instead of runner-based one</li>
<li><strong>aux_render_modes</strong> – additional visualisations to include in per-episode rendering summary</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last">
<li><p class="first">On <cite>time_flat</cite> arg:</p>
<blockquote>
<div><p>There are two alternatives to run RNN part of policy estimator:</p>
<ol class="loweralpha simple">
<li><dl class="first docutils">
<dt>Feed initial RNN state for every experience frame in rollout</dt>
<dd>(those are stored anyway if we want random memory repaly sampling) and do single time-step RNN
advance for all experiences in a batch; this is when time_flat=True;</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reshape incoming batch after convolution part of network in time-wise fashion</dt>
<dd>for every rollout in a batch i.e. batch_size=number_of_rollouts and
rnn_timesteps=max_rollout_length. In this case we need to feed initial rnn_states
for rollouts only. There is some little extra work to pad rollouts to max_time_size
and feed true rollout lengths to rnn. Thus, when time_flat=False, we unroll RNN in
specified number of time-steps for every rollout.</dd>
</dl>
</li>
</ol>
<p>Both options has pros and cons:</p>
<dl class="docutils">
<dt>Unrolling dynamic RNN is computationally more expensive but gives clearly faster convergence,</dt>
<dd><p class="first last">[possibly] due to the fact that RNN states for 2nd, 3rd, … frames
of rollouts are computed using updated policy estimator, which is supposed to be
closer to optimal one. When time_flattened, every time-step uses RNN states computed
when rollout was collected (i.e. by behavioral policy estimator with older
parameters).</p>
</dd>
<dt>Nevertheless, time_flat:</dt>
<dd><ul class="first simple">
<li>allows use of static RNN;</li>
<li>one can safely shuffle training batch or mix on-policy and off-policy data in single mini-batch,</li>
</ul>
<p class="last">ensuring iid property;
- allowing second-order derivatives which is impossible in current tf dynamic RNN implementation as
it uses tf.while_loop internally;
- computationally cheaper;</p>
</dd>
</dl>
</div></blockquote>
</li>
</ul>
</div>
<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.get_data">
<code class="descname">get_data</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.get_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.get_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Collect rollouts from every environment.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">dictionary of lists of data streams collected from every runner</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.get_sample_config">
<code class="descname">get_sample_config</code><span class="sig-paren">(</span><em>_new_trial=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.get_sample_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.get_sample_config" title="Permalink to this definition">¶</a></dt>
<dd><p>WARNING: _new_trial=True is quick fix, TODO: fix it properly!
Returns environment configuration parameters for next episode to sample.
By default is simple stateful iterator,
works correctly with <cite>DTGymDataset</cite> data class, repeating cycle:</p>
<blockquote>
<div><ul class="simple">
<li>sample <cite>num_train_episodes</cite> from train data,</li>
<li>sample <cite>num_test_episodes</cite> from test data.</li>
</ul>
</div></blockquote>
<p>Convention: supposed to override dummy method of local policy instance, see inside ._make_policy() method</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">configuration dictionary of type <cite>btgym.datafeed.base.EnvResetConfig</cite></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.start">
<code class="descname">start</code><span class="sig-paren">(</span><em>sess</em>, <em>summary_writer</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.start"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.start" title="Permalink to this definition">¶</a></dt>
<dd><p>Executes all initializing operations,
starts environment runner[s].
Supposed to be called by parent worker just before training loop starts.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sess</strong> – tf session object.</li>
<li><strong>kwargs</strong> – not used by default.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.process_data">
<code class="descname">process_data</code><span class="sig-paren">(</span><em>sess</em>, <em>data</em>, <em>is_train</em>, <em>pi</em>, <em>pi_prime=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.process_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.process_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Processes data, composes train step feed dictionary.
:param sess: tf session obj.
:param pi: policy to feed
:param pi_prime: optional policy to feed
:param data: data dictionary
:type data: dict
:param is_train: is data provided are train or test</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><strong>feed_dict</strong> – train step feed dictionary</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)">dict</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.process_summary">
<code class="descname">process_summary</code><span class="sig-paren">(</span><em>sess</em>, <em>data</em>, <em>model_data=None</em>, <em>step=None</em>, <em>episode=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.process_summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.process_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Fetches and writes summary data from <cite>data</cite> and <cite>model_data</cite>.
:param sess: tf summary obj.
:param data: thread_runner rollouts and metadata
:type data: dict
:param model_data: model summary data
:type model_data: dict
:param step: int, global step or None
:param episode: int, global episode number or None</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>sess</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Main train step method wrapper. Override if needed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sess</strong> (<em>tensorflow.Session</em>) – tf session obj.</li>
<li><strong>kwargs</strong> – any</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.aac.Unreal">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">Unreal</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#Unreal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.Unreal" title="Permalink to this definition">¶</a></dt>
<dd><p>Unreal: Asynchronous Advantage Actor Critic with auxiliary control tasks.</p>
<p>Auxiliary tasks implementation borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:
<a class="reference external" href="https://miyosuda.github.io/">https://miyosuda.github.io/</a>
<a class="reference external" href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></p>
<p>Original A3C code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Papers:
<a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a>
<a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></p>
<p>See BaseAAC class args for details:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance or list of instances</li>
<li><strong>task</strong> – int, parent worker id</li>
<li><strong>policy_config</strong> – policy estimator class and configuration dictionary</li>
<li><strong>log_level</strong> – int, logbook.level</li>
<li><strong>on_policy_loss</strong> – callable returning tensor holding on_policy training loss graph and summaries</li>
<li><strong>off_policy_loss</strong> – callable returning tensor holding off_policy training loss graph and summaries</li>
<li><strong>vr_loss</strong> – callable returning tensor holding value replay loss graph and summaries</li>
<li><strong>rp_loss</strong> – callable returning tensor holding reward prediction loss graph and summaries</li>
<li><strong>pc_loss</strong> – callable returning tensor holding pixel_control loss graph and summaries</li>
<li><strong>random_seed</strong> – int or None</li>
<li><strong>model_gamma</strong> – scalar, gamma discount factor</li>
<li><strong>model_gae_lambda</strong> – scalar, GAE lambda</li>
<li><strong>model_beta</strong> – entropy regularization beta, scalar or [high_bound, low_bound] for log_uniform.</li>
<li><strong>opt_max_env_steps</strong> – int, total number of environment steps to run training on.</li>
<li><strong>opt_decay_steps</strong> – int, learn ratio decay steps, in number of environment steps.</li>
<li><strong>opt_end_learn_rate</strong> – scalar, final learn rate</li>
<li><strong>opt_learn_rate</strong> – start learn rate, scalar or [high_bound, low_bound] for log_uniform distr.</li>
<li><strong>opt_decay</strong> – scalar, optimizer decay, if apll.</li>
<li><strong>opt_momentum</strong> – scalar, optimizer momentum, if apll.</li>
<li><strong>opt_epsilon</strong> – scalar, optimizer epsilon</li>
<li><strong>rollout_length</strong> – int, on-policy rollout length</li>
<li><strong>time_flat</strong> – bool, flatten rnn time-steps in rollouts while training - see <cite>Notes</cite> below</li>
<li><strong>episode_train_test_cycle</strong> – tuple or list as (train_number, test_number), def=(1,0): enables infinite
loop such as: run <cite>train_number</cite> of train data episodes,
than <cite>test_number</cite> of test data episodes, repeat. Should be consistent
with provided dataset parameters (test data should exist if <cite>test_number &gt; 0</cite>)</li>
<li><strong>episode_summary_freq</strong> – int, write episode summary for every i’th episode</li>
<li><strong>env_render_freq</strong> – int, write environment rendering summary for every i’th train step</li>
<li><strong>model_summary_freq</strong> – int, write model summary for every i’th train step</li>
<li><strong>test_mode</strong> – bool, True: Atari, False: BTGym</li>
<li><strong>replay_memory_size</strong> – int, in number of experiences</li>
<li><strong>replay_batch_size</strong> – int, mini-batch size for off-policy training, def = 1</li>
<li><strong>replay_rollout_length</strong> – int off-policy rollout length by def. equals on_policy_rollout_length</li>
<li><strong>use_off_policy_aac</strong> – bool, use full AAC off-policy loss instead of Value-replay</li>
<li><strong>use_reward_prediction</strong> – bool, use aux. off-policy reward prediction task</li>
<li><strong>use_pixel_control</strong> – bool, use aux. off-policy pixel control task</li>
<li><strong>use_value_replay</strong> – bool, use aux. off-policy value replay task (not used if use_off_policy_aac=True)</li>
<li><strong>rp_lambda</strong> – reward prediction loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>pc_lambda</strong> – pixel control loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>vr_lambda</strong> – value replay loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>off_aac_lambda</strong> – off-policy AAC loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>gamma_pc</strong> – NOT USED</li>
<li><strong>rp_reward_threshold</strong> – scalar, reward prediction classification threshold, above which reward is ‘non-zero’</li>
<li><strong>rp_sequence_size</strong> – int, reward prediction sample size, in number of experiences</li>
<li><strong>clip_epsilon</strong> – scalar, PPO: surrogate L^clip epsilon</li>
<li><strong>num_epochs</strong> – int, num. of SGD runs for every train step, val. &gt; 1 should be used with caution.</li>
<li><strong>pi_prime_update_period</strong> – int, PPO: pi to pi_old update period in number of train steps, def: 1</li>
<li><strong>_use_target_policy</strong> – bool, PPO: use target policy (aka pi_old), delayed by <cite>pi_prime_update_period</cite> delay</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last">
<li><p class="first">On <cite>time_flat</cite> arg:</p>
<blockquote>
<div><p>There are two alternatives to run RNN part of policy estimator:</p>
<ol class="loweralpha simple">
<li><dl class="first docutils">
<dt>Feed initial RNN state for every experience frame in rollout</dt>
<dd>(those are stored anyway if we want random memory repaly sampling) and do single time-step RNN
advance for all experiences in a batch; this is when time_flat=True;</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reshape incoming batch after convolution part of network in time-wise fashion</dt>
<dd>for every rollout in a batch i.e. batch_size=number_of_rollouts and
rnn_timesteps=max_rollout_length. In this case we need to feed initial rnn_states
for rollouts only. There is some little extra work to pad rollouts to max_time_size
and feed true rollout lengths to rnn. Thus, when time_flat=False, we unroll RNN in
specified number of time-steps for every rollout.</dd>
</dl>
</li>
</ol>
<p>Both options has pros and cons:</p>
<dl class="docutils">
<dt>Unrolling dynamic RNN is computationally more expensive but gives clearly faster convergence,</dt>
<dd><p class="first last">[possibly] due to the fact that RNN states for 2nd, 3rd, … frames
of rollouts are computed using updated policy estimator, which is supposed to be
closer to optimal one. When time_flattened, every time-step uses RNN states computed
when rollout was collected (i.e. by behavioral policy estimator with older
parameters).</p>
</dd>
<dt>Nevertheless, time_flatting can be interesting</dt>
<dd><p class="first last">because one can safely shuffle training batch or mix on-policy and off-policy data in single mini-batch,
ensuring iid property and allowing, say, proper batch normalisation (this has yet to be tested).</p>
</dd>
</dl>
</div></blockquote>
</li>
</ul>
</div>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.aac.A3C">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">A3C</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#A3C"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.A3C" title="Permalink to this definition">¶</a></dt>
<dd><p>Vanilla Asynchronous Advantage Actor Critic algorithm.</p>
<p>Based on original code taken from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Paper: <a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a></p>
<p>A3C args. is a subset of BaseAAC arguments, see <cite>BaseAAC</cite> class for descriptions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – </li>
<li><strong>task</strong> – </li>
<li><strong>policy_config</strong> – </li>
<li><strong>log</strong> – </li>
<li><strong>random_seed</strong> – </li>
<li><strong>model_gamma</strong> – </li>
<li><strong>model_gae_lambda</strong> – </li>
<li><strong>model_beta</strong> – </li>
<li><strong>opt_max_env_steps</strong> – </li>
<li><strong>opt_decay_steps</strong> – </li>
<li><strong>opt_end_learn_rate</strong> – </li>
<li><strong>opt_learn_rate</strong> – </li>
<li><strong>opt_decay</strong> – </li>
<li><strong>opt_momentum</strong> – </li>
<li><strong>opt_epsilon</strong> – </li>
<li><strong>rollout_length</strong> – </li>
<li><strong>episode_summary_freq</strong> – </li>
<li><strong>env_render_freq</strong> – </li>
<li><strong>model_summary_freq</strong> – </li>
<li><strong>test_mode</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.aac.PPO">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">PPO</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#PPO"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.PPO" title="Permalink to this definition">¶</a></dt>
<dd><p>AAC with Proximal Policy Optimization surrogate L^Clip loss,
optionally augmented with auxiliary control tasks.</p>
<p>paper:
<a class="reference external" href="https://arxiv.org/pdf/1707.06347.pdf">https://arxiv.org/pdf/1707.06347.pdf</a></p>
<p>Based on PPO-SGD code from OpenAI <cite>Baselines</cite> repository under MIT licence:
<a class="reference external" href="https://github.com/openai/baselines">https://github.com/openai/baselines</a></p>
<p>Async. framework code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<blockquote>
<div>PPO args. is a subset of BaseAAC arguments, see <cite>BaseAAC</cite> class for descriptions.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – </li>
<li><strong>task</strong> – </li>
<li><strong>policy_config</strong> – </li>
<li><strong>log_level</strong> – </li>
<li><strong>vr_loss</strong> – </li>
<li><strong>rp_loss</strong> – </li>
<li><strong>pc_loss</strong> – </li>
<li><strong>random_seed</strong> – </li>
<li><strong>model_gamma</strong> – </li>
<li><strong>model_gae_lambda</strong> – </li>
<li><strong>model_beta</strong> – </li>
<li><strong>opt_max_env_steps</strong> – </li>
<li><strong>opt_decay_steps</strong> – </li>
<li><strong>opt_end_learn_rate</strong> – </li>
<li><strong>opt_learn_rate</strong> – </li>
<li><strong>opt_decay</strong> – </li>
<li><strong>opt_momentum</strong> – </li>
<li><strong>opt_epsilon</strong> – </li>
<li><strong>rollout_length</strong> – </li>
<li><strong>episode_summary_freq</strong> – </li>
<li><strong>env_render_freq</strong> – </li>
<li><strong>model_summary_freq</strong> – </li>
<li><strong>test_mode</strong> – </li>
<li><strong>replay_memory_size</strong> – </li>
<li><strong>replay_rollout_length</strong> – </li>
<li><strong>use_off_policy_aac</strong> – </li>
<li><strong>use_reward_prediction</strong> – </li>
<li><strong>use_pixel_control</strong> – </li>
<li><strong>use_value_replay</strong> – </li>
<li><strong>rp_lambda</strong> – </li>
<li><strong>pc_lambda</strong> – </li>
<li><strong>vr_lambda</strong> – </li>
<li><strong>off_aac_lambda</strong> – </li>
<li><strong>rp_reward_threshold</strong> – </li>
<li><strong>rp_sequence_size</strong> – </li>
<li><strong>clip_epsilon</strong> – </li>
<li><strong>num_epochs</strong> – </li>
<li><strong>pi_prime_update_period</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.rollout">
<span id="btgym-algorithms-rollout-module"></span><h2>btgym.algorithms.rollout module<a class="headerlink" href="#module-btgym.algorithms.rollout" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="btgym.algorithms.rollout.make_data_getter">
<code class="descclassname">btgym.algorithms.rollout.</code><code class="descname">make_data_getter</code><span class="sig-paren">(</span><em>queue</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#make_data_getter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.make_data_getter" title="Permalink to this definition">¶</a></dt>
<dd><p>Data stream getter constructor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>queue</strong> – instance of <cite>Queue</cite> class to get rollouts from.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">callable, returning dictionary of data.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.rollout.Rollout">
<em class="property">class </em><code class="descclassname">btgym.algorithms.rollout.</code><code class="descname">Rollout</code><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout" title="Permalink to this definition">¶</a></dt>
<dd><p>Experience rollout as [nested] dictionary of lists of ndarrays, tuples and rnn states.</p>
<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>values</em>, <em>_struct=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds single experience frame to rollout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>values</strong> – [nested] dictionary of values.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.add_memory_sample">
<code class="descname">add_memory_sample</code><span class="sig-paren">(</span><em>sample</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.add_memory_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.add_memory_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given replay memory sample as list of experience-dictionaries of <cite>length</cite>,
converts it to rollout of same <cite>length</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>gamma</em>, <em>gae_lambda=1.0</em>, <em>size=None</em>, <em>time_flat=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts single-trajectory rollout of experiences to dictionary of ready-to-feed arrays.
Computes rollout returns and the advantages.
Pads with zeroes to desired length, if size arg is given.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>gamma</strong> – discount factor</li>
<li><strong>gae_lambda</strong> – GAE lambda</li>
<li><strong>size</strong> – if given and time_flat=False, pads outputs with zeroes along <a href="#id11"><span class="problematic" id="id12">`</span></a>time’ dim. to exact ‘size’.</li>
<li><strong>time_flat</strong> – reduce time dimension to 1 step by stacking all experiences along batch dimension.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>[1, time_size, depth] or [1, size, depth] if not time_flatten and <cite>size</cite> is not/given, with single
<cite>context</cite> entry for entire trajectory, i.e. of size [1, context_depth];</p>
<p>[batch_size, 1, depth], if time_flatten, with batch_size = time_size and <cite>context</cite> entry for
every experience frame, i.e. of size [batch_size, context_depth].</p>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">batch as [nested] dictionary of np.arrays, tuples and LSTMStateTuples. of size</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.process_rp">
<code class="descname">process_rp</code><span class="sig-paren">(</span><em>reward_threshold=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.process_rp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.process_rp" title="Permalink to this definition">¶</a></dt>
<dd><p>Processes rollout process()-alike and estimates reward prediction target for first n-1 frames.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>reward_threshold</strong> – reward values such as <a href="#id13"><span class="problematic" id="id14">|r|</span></a>&gt; reward_threshold are classified as neg. or pos.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Processed batch with size reduced by one and with extra <cite>rp_target</cite> key
holding one hot encodings for classes {zero, positive, negative}.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.get_frame">
<code class="descname">get_frame</code><span class="sig-paren">(</span><em>idx</em>, <em>_struct=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.get_frame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.get_frame" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts single experience from rollout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>idx</strong> – experience position</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">frame as [nested] dictionary</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.pop_frame">
<code class="descname">pop_frame</code><span class="sig-paren">(</span><em>idx</em>, <em>_struct=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.pop_frame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.pop_frame" title="Permalink to this definition">¶</a></dt>
<dd><p>Pops single experience from rollout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>idx</strong> – experience position</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">frame as [nested] dictionary</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.memory">
<span id="btgym-algorithms-memory-module"></span><h2>btgym.algorithms.memory module<a class="headerlink" href="#module-btgym.algorithms.memory" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.memory.Memory">
<em class="property">class </em><code class="descclassname">btgym.algorithms.memory.</code><code class="descname">Memory</code><span class="sig-paren">(</span><em>history_size</em>, <em>max_sample_size</em>, <em>priority_sample_size</em>, <em>log_level=13</em>, <em>rollout_provider=None</em>, <em>task=-1</em>, <em>reward_threshold=0.1</em>, <em>use_priority_sampling=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Replay memory with rebalanced replay based on reward value.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">must be filled up before calling sampling methods.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>history_size</strong> – number of experiences stored;</li>
<li><strong>max_sample_size</strong> – maximum allowed sample size (e.g. off-policy rollout length);</li>
<li><strong>priority_sample_size</strong> – sample size of priority_sample() method</li>
<li><strong>log_level</strong> – int, logbook.level;</li>
<li><strong>rollout_provider</strong> – callable returning list of Rollouts NOT USED</li>
<li><strong>task</strong> – parent worker id;</li>
<li><strong>reward_threshold</strong> – if <a href="#id15"><span class="problematic" id="id16">|experience.reward|</span></a> &gt; reward_threshold: experience is saved as ‘prioritized’;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.memory.Memory.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>frame</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends single experience frame to memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>frame</strong> – dictionary of values.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.add_rollout">
<code class="descname">add_rollout</code><span class="sig-paren">(</span><em>rollout</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.add_rollout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.add_rollout" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds frames from given rollout to memory with respect to episode continuation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>rollout</strong> – <cite>Rollout</cite> instance.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.fill">
<code class="descname">fill</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.fill"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills replay memory with initial experiences. NOT USED.
Supposed to be called by parent worker() just before training begins.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>rollout_getter</strong> – callable, returning list of Rollouts.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.sample_uniform">
<code class="descname">sample_uniform</code><span class="sig-paren">(</span><em>sequence_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.sample_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.sample_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Uniformly samples sequence of successive frames of size <cite>sequence_size</cite> or less (~off-policy rollout).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sequence_size</strong> – maximum sample size.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">instance of Rollout of size &lt;= sequence_size.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory._sample_priority">
<code class="descname">_sample_priority</code><span class="sig-paren">(</span><em>size=None</em>, <em>exact_size=False</em>, <em>skewness=2</em>, <em>sample_attempts=100</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory._sample_priority"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory._sample_priority" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements rebalanced replay.
Samples sequence of successive frames from distribution skewed by means of reward of last sample frame.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> – sample size, must be &lt;= self.max_sample_size;</li>
<li><strong>exact_size</strong> – whether accept sample with size less than ‘size’
or re-sample to get sample of exact size (used for reward prediction task);</li>
<li><strong>skewness</strong> – int&gt;=1, sampling probability denominator, such as probability of sampling sequence with
last frame having non-zero reward is: p[non_zero]=1/skewness;</li>
<li><strong>sample_attempts</strong> – if exact_size=True, sets number of re-sampling attempts
to get sample of continuous experiences (no <cite>Terminal</cite> frames inside except last one);
if number is reached - sample returned ‘as is’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">instance of Rollout().</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.envs">
<span id="btgym-algorithms-envs-module"></span><h2>btgym.algorithms.envs module<a class="headerlink" href="#module-btgym.algorithms.envs" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.envs.AtariRescale42x42">
<em class="property">class </em><code class="descclassname">btgym.algorithms.envs.</code><code class="descname">AtariRescale42x42</code><span class="sig-paren">(</span><em>env_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/envs.html#AtariRescale42x42"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.envs.AtariRescale42x42" title="Permalink to this definition">¶</a></dt>
<dd><p>Gym wrapper, pipes Atari into BTgym algorithms, as later expect observations to be DictSpace.
Makes Atari environment return state as dictionary with single key ‘external’ holding
normalized in [0,1] grayscale 42x42 visual output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>env_id</strong> – conventional Gym id.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="btgym.algorithms.nn.html" class="btn btn-neutral float-right" title="btgym.algorithms.nn.losses module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="btgym.datafeed.html" class="btn btn-neutral" title="btgym.datafeed package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, 2018, Andrew Muzikin.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.0.7',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>