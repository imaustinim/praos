

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>btgym.algorithms.policy.base module &mdash; BTGym 0.0.7 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="BTGym 0.0.7 documentation" href="index.html"/>
        <link rel="up" title="btgym.algorithms package" href="btgym.algorithms.html"/>
        <link rel="next" title="btgym.algorithms.runner.base module" href="btgym.algorithms.runner.html"/>
        <link rel="prev" title="btgym.algorithms.nn.losses module" href="btgym.algorithms.nn.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> BTGym
          

          
          </a>

          
            
            
              <div class="version">
                0.0.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Package Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#quickstart">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#problem-definition">Problem definition</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#environment-engine-description">Environment engine description</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#data-flow-structure">Data flow structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#a3c-framework-description">A3C framework description</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.envs.html">btgym.envs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html">btgym.dataserver module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html#module-btgym.server">btgym.server module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html#module-btgym.spaces">btgym.spaces module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.strategy.html">btgym.strategy package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.monitor.html">btgym.monitor package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.rendering.html">btgym.rendering package</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.datafeed.html">btgym.datafeed package</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="btgym.algorithms.html">btgym.algorithms package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#btgym-algorithms-nn-subpackage">btgym.algorithms.nn subpackage</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="btgym.algorithms.html#btgym-algorithms-policy-subpackage">btgym.algorithms.policy subpackage</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">btgym.algorithms.policy.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-btgym.algorithms.policy.stacked_lstm">btgym.algorithms.policy.stacked_lstm module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#btgym-algorithms-runner-subpackage">btgym.algorithms.runner subpackage</a></li>
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#module-btgym.algorithms.launcher">btgym.algorithms.launcher module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#module-btgym.algorithms.worker">btgym.algorithms.worker module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#module-btgym.algorithms.aac">btgym.algorithms.aac module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#module-btgym.algorithms.rollout">btgym.algorithms.rollout module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#module-btgym.algorithms.memory">btgym.algorithms.memory module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btgym.algorithms.html#module-btgym.algorithms.envs">btgym.algorithms.envs module</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.research.html">btgym.research package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BTGym</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="btgym.algorithms.html">btgym.algorithms package</a> &raquo;</li>
        
      <li>btgym.algorithms.policy.base module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/btgym.algorithms.policy.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-btgym.algorithms.policy.base">
<span id="btgym-algorithms-policy-base-module"></span><h1>btgym.algorithms.policy.base module<a class="headerlink" href="#module-btgym.algorithms.policy.base" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="btgym.algorithms.policy.base.BaseAacPolicy">
<em class="property">class </em><code class="descclassname">btgym.algorithms.policy.base.</code><code class="descname">BaseAacPolicy</code><span class="sig-paren">(</span><em>ob_space</em>, <em>ac_space</em>, <em>rp_sequence_size</em>, <em>lstm_class=&lt;class 'tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell'&gt;</em>, <em>lstm_layers=(256</em>, <em>)</em>, <em>action_dp_alpha=200.0</em>, <em>aux_estimate=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/base.html#BaseAacPolicy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.base.BaseAacPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>Base advantage actor-critic Convolution-LSTM policy estimator with auxiliary control tasks for
discrete or nested discrete action spaces.</p>
<p>Papers:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a>
<a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></div></blockquote>
<p>Defines [partially shared] on/off-policy networks for estimating  action-logits, value function,
reward and state ‘pixel_change’ predictions.
Expects multi-modal observation as array of shape <cite>ob_space</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>ob_space</strong> – instance of btgym.spaces.DictSpace</li>
<li><strong>ac_space</strong> – instance of btgym.spaces.ActionDictSpace</li>
<li><strong>rp_sequence_size</strong> – reward prediction sample length</li>
<li><strong>lstm_class</strong> – tf.nn.lstm class</li>
<li><strong>lstm_layers</strong> – tuple of LSTM layers sizes</li>
<li><strong>aux_estimate</strong> – bool, if True - add auxiliary tasks estimations to self.callbacks dictionary</li>
<li><strong>time_flat</strong> – bool, if True - use static rnn, dynamic otherwise</li>
<li><strong>not used</strong> (<em>**kwargs</em>) – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.policy.base.BaseAacPolicy.get_initial_features">
<code class="descname">get_initial_features</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/base.html#BaseAacPolicy.get_initial_features"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.base.BaseAacPolicy.get_initial_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns initial context.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">LSTM zero-state tuple.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.policy.base.BaseAacPolicy.act">
<code class="descname">act</code><span class="sig-paren">(</span><em>observation</em>, <em>lstm_state</em>, <em>last_action</em>, <em>last_reward</em>, <em>deterministic=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/base.html#BaseAacPolicy.act"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.base.BaseAacPolicy.act" title="Permalink to this definition">¶</a></dt>
<dd><p>Emits action.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>observation</strong> – dictionary containing single observation</li>
<li><strong>lstm_state</strong> – lstm context value</li>
<li><strong>last_action</strong> – action value from previous step</li>
<li><strong>last_reward</strong> – reward value previous step</li>
<li><strong>deterministic</strong> – bool, it True - act deterministically,
use random sampling otherwise (default);
effective for discrete action sapce only (TODO: continious)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Action as dictionary of several action encodings, actions logits, V-fn value, output RNN state</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.policy.base.BaseAacPolicy.get_value">
<code class="descname">get_value</code><span class="sig-paren">(</span><em>observation</em>, <em>lstm_state</em>, <em>last_action</em>, <em>last_reward</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/base.html#BaseAacPolicy.get_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.base.BaseAacPolicy.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimates policy V-function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>observation</strong> – single observation value</li>
<li><strong>lstm_state</strong> – lstm context value</li>
<li><strong>last_action</strong> – action value from previous step</li>
<li><strong>last_reward</strong> – reward value from previous step</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">V-function value</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.policy.base.BaseAacPolicy.get_pc_target">
<code class="descname">get_pc_target</code><span class="sig-paren">(</span><em>state</em>, <em>last_state</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/base.html#BaseAacPolicy.get_pc_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.base.BaseAacPolicy.get_pc_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimates pixel-control task target.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>state</strong> – single observation value</li>
<li><strong>last_state</strong> – single observation value</li>
<li><strong>**kwargs</strong> – not used</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Estimated absolute difference between two subsampled states.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="btgym.algorithms.policy.base.BaseAacPolicy.get_sample_config">
<em class="property">static </em><code class="descname">get_sample_config</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/base.html#BaseAacPolicy.get_sample_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.base.BaseAacPolicy.get_sample_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Dummy implementation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">default data sample configuration dictionary <cite>btgym.datafeed.base.EnvResetConfig</cite></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.policy.base.Aac1dPolicy">
<em class="property">class </em><code class="descclassname">btgym.algorithms.policy.base.</code><code class="descname">Aac1dPolicy</code><span class="sig-paren">(</span><em>ob_space</em>, <em>ac_space</em>, <em>rp_sequence_size</em>, <em>lstm_class=&lt;class 'tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell'&gt;</em>, <em>lstm_layers=(256</em>, <em>)</em>, <em>action_dp_alpha=200.0</em>, <em>aux_estimate=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/base.html#Aac1dPolicy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.base.Aac1dPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>AAC policy for one-dimensional signal obs. state.</p>
<p>Defines [partially shared] on/off-policy networks for estimating  action-logits, value function,
reward and state ‘pixel_change’ predictions.
Expects bi-modal observation as dict: <cite>external</cite>, <cite>internal</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>ob_space</strong> – dictionary of observation state shapes</li>
<li><strong>ac_space</strong> – discrete action space shape (length)</li>
<li><strong>rp_sequence_size</strong> – reward prediction sample length</li>
<li><strong>lstm_class</strong> – tf.nn.lstm class</li>
<li><strong>lstm_layers</strong> – tuple of LSTM layers sizes</li>
<li><strong>aux_estimate</strong> – (bool), if True - add auxiliary tasks estimations to self.callbacks dictionary.</li>
<li><strong>not used</strong> (<em>**kwargs</em>) – </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.policy.stacked_lstm">
<span id="btgym-algorithms-policy-stacked-lstm-module"></span><h1>btgym.algorithms.policy.stacked_lstm module<a class="headerlink" href="#module-btgym.algorithms.policy.stacked_lstm" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="btgym.algorithms.policy.stacked_lstm.StackedLstmPolicy">
<em class="property">class </em><code class="descclassname">btgym.algorithms.policy.stacked_lstm.</code><code class="descname">StackedLstmPolicy</code><span class="sig-paren">(</span><em>ob_space</em>, <em>ac_space</em>, <em>rp_sequence_size</em>, <em>state_encoder_class_ref=&lt;function conv_2d_network&gt;</em>, <em>lstm_class_ref=&lt;class 'tensorflow.contrib.rnn.python.ops.rnn_cell.LayerNormBasicLSTMCell'&gt;</em>, <em>lstm_layers=(256</em>, <em>256)</em>, <em>linear_layer_ref=&lt;function noisy_linear&gt;</em>, <em>share_encoder_params=False</em>, <em>dropout_keep_prob=1.0</em>, <em>action_dp_alpha=200.0</em>, <em>aux_estimate=False</em>, <em>encode_internal_state=False</em>, <em>static_rnn=True</em>, <em>shared_p_v=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/stacked_lstm.html#StackedLstmPolicy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.stacked_lstm.StackedLstmPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>Conv.-Stacked_LSTM policy, based on <cite>NAV A3C agent</cite> architecture from</p>
<p><cite>LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS</cite> by Mirowski et all. and</p>
<p><cite>LEARNING TO REINFORCEMENT LEARN</cite> by JX Wang et all.</p>
<p>Papers:</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1611.03673.pdf">https://arxiv.org/pdf/1611.03673.pdf</a></p>
<p><a class="reference external" href="https://arxiv.org/pdf/1611.05763.pdf">https://arxiv.org/pdf/1611.05763.pdf</a></p>
<p>Defines [partially shared] on/off-policy networks for estimating  action-logits, value function,
reward and state ‘pixel_change’ predictions.
Expects multi-modal observation as array of shape <cite>ob_space</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>ob_space</strong> – instance of btgym.spaces.DictSpace</li>
<li><strong>ac_space</strong> – instance of btgym.spaces.ActionDictSpace</li>
<li><strong>rp_sequence_size</strong> – reward prediction sample length</li>
<li><strong>lstm_class_ref</strong> – tf.nn.lstm class to use</li>
<li><strong>lstm_layers</strong> – tuple of LSTM layers sizes</li>
<li><strong>linear_layer_ref</strong> – linear layer class to use</li>
<li><strong>share_encoder_params</strong> – bool, whether to share encoder parameters for every ‘external’ data stream</li>
<li><strong>dropout_keep_prob</strong> – in (0, 1] dropout regularisation parameter</li>
<li><strong>action_dp_alpha</strong> – </li>
<li><strong>aux_estimate</strong> – (bool), if True - add auxiliary tasks estimations to self.callbacks dictionary</li>
<li><strong>encode_internal_state</strong> – use encoder over ‘internal’ part of observation space</li>
<li><strong>static_rnn</strong> – (bool), it True - use static rnn graph, dynamic otherwise</li>
<li><strong>not used</strong> (<em>**kwargs</em>) – </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.policy.stacked_lstm.AacStackedRL2Policy">
<em class="property">class </em><code class="descclassname">btgym.algorithms.policy.stacked_lstm.</code><code class="descname">AacStackedRL2Policy</code><span class="sig-paren">(</span><em>lstm_2_init_period=50</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/stacked_lstm.html#AacStackedRL2Policy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.stacked_lstm.AacStackedRL2Policy" title="Permalink to this definition">¶</a></dt>
<dd><p>Attempt to implement two-level RL^2
This policy class in conjunction with DataDomain classes from btgym.datafeed
is aimed to implement RL^2 algorithm by Duan et al.</p>
<p>Paper:
<cite>FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING</cite>,
<a class="reference external" href="https://arxiv.org/pdf/1611.02779.pdf">https://arxiv.org/pdf/1611.02779.pdf</a></p>
<p>The only difference from Base policy is <cite>get_initial_features()</cite> method, which has been changed
either to reset RNN context to zero-state or return context from the end of previous episode,
depending on episode metadata received or <a href="#id1"><span class="problematic" id="id2">`</span></a>lstm_2_init_period’ parameter.</p>
<dl class="method">
<dt id="btgym.algorithms.policy.stacked_lstm.AacStackedRL2Policy.get_initial_features">
<code class="descname">get_initial_features</code><span class="sig-paren">(</span><em>state</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/policy/stacked_lstm.html#AacStackedRL2Policy.get_initial_features"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.policy.stacked_lstm.AacStackedRL2Policy.get_initial_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns RNN initial context.
RNN_1 (lower) context is reset at every call.</p>
<dl class="docutils">
<dt>RNN_2 (upper) context is reset:</dt>
<dd><ul class="first last simple">
<li>every <a href="#id3"><span class="problematic" id="id4">`</span></a>lstm_2_init_period’ episodes;</li>
<li>episode  initial <cite>state</cite> <cite>trial_num</cite> metadata has been changed form last call (new train trial started);</li>
<li>episode metatdata <cite>type</cite> is non-zero (test episode);</li>
<li>no context arg is provided (initial episode of training);</li>
<li>… else carries context on to new episode;</li>
</ul>
</dd>
</dl>
<p>Episode metadata are provided by DataTrialIterator, which is shaping Trial data distribution in this case,
and delivered through env.strategy as separate key in observation dictionary.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>state</strong> – initial episode state (result of env.reset())</li>
<li><strong>context</strong> – last previous episode RNN state (last_context of runner)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">2_RNN zero-state tuple.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><em>KeyError if [`metadata`]</em> – [<cite>trial_num</cite>,`type`] keys not found</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="btgym.algorithms.runner.html" class="btn btn-neutral float-right" title="btgym.algorithms.runner.base module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="btgym.algorithms.nn.html" class="btn btn-neutral" title="btgym.algorithms.nn.losses module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, 2018, Andrew Muzikin.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.0.7',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>